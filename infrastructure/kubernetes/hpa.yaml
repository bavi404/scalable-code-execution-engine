# =============================================================================
# Horizontal Pod Autoscaler - Container Pool
# =============================================================================
#
# Scales based on:
# 1. CPU utilization (built-in)
# 2. Custom metric: queue depth per worker
# 3. Custom metric: processing latency
#
# Requires metrics-server and prometheus-adapter for custom metrics

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: container-pool-hpa
  namespace: code-execution
  labels:
    app: code-execution
    pool: container
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: container-pool-worker
  
  minReplicas: 2
  maxReplicas: 100
  
  metrics:
    # CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    
    # Memory utilization
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    
    # Custom metric: Queue depth per worker
    - type: External
      external:
        metric:
          name: redis_stream_length
          selector:
            matchLabels:
              stream: code-execution-jobs
              pool: container
        target:
          type: AverageValue
          averageValue: "10"  # Target 10 jobs per worker
    
    # Custom metric: Processing latency P95
    - type: External
      external:
        metric:
          name: code_execution_latency_p95
          selector:
            matchLabels:
              pool: container
        target:
          type: Value
          value: "5000"  # Target P95 < 5 seconds
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        - type: Pods
          value: 4
          periodSeconds: 30
        - type: Percent
          value: 100
          periodSeconds: 30
      selectPolicy: Max
    
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60
        - type: Percent
          value: 10
          periodSeconds: 60
      selectPolicy: Min

---
# =============================================================================
# Horizontal Pod Autoscaler - MicroVM Pool
# =============================================================================

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: microvm-pool-hpa
  namespace: code-execution
  labels:
    app: code-execution
    pool: microvm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: microvm-pool-worker
  
  minReplicas: 1
  maxReplicas: 50
  
  metrics:
    # CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60  # Lower threshold for heavier workloads
    
    # Custom metric: Queue depth per worker
    - type: External
      external:
        metric:
          name: redis_stream_length
          selector:
            matchLabels:
              stream: code-execution-jobs
              pool: microvm
        target:
          type: AverageValue
          averageValue: "5"  # Lower target for MicroVM (heavier jobs)
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
        - type: Percent
          value: 50
          periodSeconds: 60
      selectPolicy: Max
    
    scaleDown:
      stabilizationWindowSeconds: 600  # 10 minutes (VMs are expensive to recreate)
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120
      selectPolicy: Min

---
# =============================================================================
# Prometheus Adapter Configuration (for custom metrics)
# =============================================================================
# Deploy this ConfigMap if using prometheus-adapter

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-adapter-config
  namespace: monitoring
data:
  config.yaml: |
    rules:
      # Redis stream length metric
      - seriesQuery: 'redis_stream_length{stream="code-execution-jobs"}'
        resources:
          overrides:
            namespace:
              resource: namespace
        name:
          matches: "^(.*)$"
          as: "redis_stream_length"
        metricsQuery: 'sum(redis_stream_length{stream="code-execution-jobs",pool="<<.Pool>>"}) by (pool)'
      
      # Execution latency P95
      - seriesQuery: 'code_execution_duration_seconds{quantile="0.95"}'
        resources:
          overrides:
            namespace:
              resource: namespace
        name:
          matches: "^(.*)$"
          as: "code_execution_latency_p95"
        metricsQuery: 'histogram_quantile(0.95, sum(rate(code_execution_duration_seconds_bucket{pool="<<.Pool>>"}[5m])) by (le, pool)) * 1000'
      
      # Queue depth per worker
      - seriesQuery: 'redis_stream_pending{stream="code-execution-jobs"}'
        resources:
          overrides:
            namespace:
              resource: namespace
        name:
          matches: "^(.*)$"
          as: "queue_depth_per_worker"
        metricsQuery: |
          sum(redis_stream_length{stream="code-execution-jobs",pool="<<.Pool>>"}) 
          / 
          count(kube_pod_status_ready{pod=~".*-pool-worker.*",condition="true"})

---
# =============================================================================
# PodDisruptionBudget
# =============================================================================

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: container-pool-pdb
  namespace: code-execution
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: code-execution
      pool: container

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: microvm-pool-pdb
  namespace: code-execution
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: code-execution
      pool: microvm

---
# =============================================================================
# Vertical Pod Autoscaler (optional - for right-sizing)
# =============================================================================

apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: container-pool-vpa
  namespace: code-execution
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: container-pool-worker
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: worker
        minAllowed:
          cpu: "250m"
          memory: "512Mi"
        maxAllowed:
          cpu: "4000m"
          memory: "8Gi"
        controlledResources: ["cpu", "memory"]

